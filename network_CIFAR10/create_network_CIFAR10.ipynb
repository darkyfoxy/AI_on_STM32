{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "create_network_CIFAR10.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXnDTL32PCMi"
      },
      "source": [
        "import torch\n",
        "import torchvision \n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxpiDjskY-3B"
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.1307, 0.1307, 0.1307), (0.3081, 0.3081, 0.3081))])\n",
        "\n",
        "transform_flip = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.RandomHorizontalFlip(p=1.0),\n",
        "     transforms.Normalize((0.1307, 0.1307, 0.1307), (0.3081, 0.3081, 0.3081))])\n",
        "\n",
        "batch_size = 64\n",
        "trainset = datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainset_flip = datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform_flip)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(torch.utils.data.ConcatDataset([trainset, trainset_flip]), batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbspvbWabSFY"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
        "        self.norm1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 32, 3, padding=1)\n",
        "        self.norm2 = nn.BatchNorm2d(32)\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        self.drop1 = nn.Dropout(0.2)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.norm3 = nn.BatchNorm2d(64)\n",
        "        self.conv4 = nn.Conv2d(64, 64, 3, padding=1)\n",
        "        self.norm4 = nn.BatchNorm2d(64)\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "        self.drop2 = nn.Dropout(0.3)\n",
        "\n",
        "        self.conv5 = nn.Conv2d(64, 64, 3, padding=1)\n",
        "        self.norm5 = nn.BatchNorm2d(64)\n",
        "        self.conv6 = nn.Conv2d(64, 64, 3, padding=1)\n",
        "        self.norm6 = nn.BatchNorm2d(64)\n",
        "        self.pool3 = nn.MaxPool2d(2, 2)\n",
        "        self.drop3 = nn.Dropout(0.4)\n",
        "\n",
        "        self.fc1 = nn.Linear(64* 4* 4, 128)\n",
        "        self.norm7 = nn.BatchNorm1d(128)\n",
        "        self.drop4 = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.norm1(F.relu(self.conv1(x)))\n",
        "        x = self.norm2(F.relu(self.conv2(x)))\n",
        "        x = self.drop1(self.pool1(x))\n",
        "\n",
        "        x = self.norm3(F.relu(self.conv3(x)))\n",
        "        x = self.norm4(F.relu(self.conv4(x)))\n",
        "        x = self.drop2(self.pool2(x))\n",
        "\n",
        "        x = self.norm5(F.relu(self.conv5(x)))\n",
        "        x = self.norm6(F.relu(self.conv6(x)))\n",
        "        x = self.drop3(self.pool3(x))\n",
        "        # print(x.shape)\n",
        "        x = torch.flatten(x, 1) \n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.drop4(self.norm7(x))\n",
        "        x = F.softmax(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "net = Net()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZbb_FUebvXj"
      },
      "source": [
        "epochs = 20\n",
        "log_interval = 16\n",
        "accuracy_hist = []\n",
        "loss_hist = []\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    accuracy_avr = 0\n",
        "    loss_avr = 0\n",
        "    for batch_idx, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        net_out = net(inputs)\n",
        "        accuracy = (torch.argmax(net_out, 1) == labels).sum()/trainloader.batch_size\n",
        "\n",
        "        loss = criterion(net_out, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        accuracy_avr += accuracy\n",
        "        loss_avr += loss.data\n",
        "\n",
        "        if batch_idx % log_interval == 0 and batch_idx != 0:\n",
        "          accuracy_hist.append(accuracy_avr/log_interval)\n",
        "          loss_hist.append(loss_avr/log_interval)\n",
        "\n",
        "          if batch_idx % (log_interval) == 0:\n",
        "            print('epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} \\tAccuracy: {:.6f} '.format(\n",
        "                epoch, batch_idx * len(inputs), len(trainloader.dataset),\n",
        "                        100. * batch_idx / len(trainloader), loss_avr/log_interval, accuracy_avr/log_interval))\n",
        "          accuracy_avr = 0\n",
        "          loss_avr = 0\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JfIGO3iesY9"
      },
      "source": [
        "fig = plt.figure(figsize = (20, 10))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(accuracy_hist)\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(loss_hist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CU4AYjKdSQx"
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        # calculate outputs by running images through the network\n",
        "        outputs = net(images)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "    100 * correct / total))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-m4fFpndhtt"
      },
      "source": [
        "# prepare to count predictions for each class\n",
        "correct_pred = {classname: 0 for classname in classes}\n",
        "total_pred = {classname: 0 for classname in classes}\n",
        "\n",
        "# again no gradients needed\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "        # collect the correct predictions for each class\n",
        "        for label, prediction in zip(labels, predictions):\n",
        "            if label == prediction:\n",
        "                correct_pred[classes[label]] += 1\n",
        "            total_pred[classes[label]] += 1\n",
        "\n",
        "\n",
        "# print accuracy for each class\n",
        "for classname, correct_count in correct_pred.items():\n",
        "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "    print(\"Accuracy for class {:5s} is: {:.1f} %\".format(classname,\n",
        "                                                   accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkPiPb2ld5pt"
      },
      "source": [
        "net.eval() \n",
        "\n",
        "dummy_input = torch.randn(1, 3, 32, 32, requires_grad=True)  \n",
        "  \n",
        "torch.onnx.export(net, dummy_input, \"network.onnx\",  \n",
        "      export_params=True, opset_version=10, do_constant_folding=True,\n",
        "      input_names = ['modelInput'], output_names = ['modelOutput']) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yask0qIJnCVw"
      },
      "source": [
        "test_size = 200\n",
        "input = np.zeros([test_size, 32, 32, 3])\n",
        "output = np.zeros([test_size, 10])\n",
        "for i in range(test_size):\n",
        "  for c, chanal in enumerate(np.array(testloader.dataset[i][0])):\n",
        "   for l, line in enumerate(chanal):\n",
        "     for v, item in enumerate(line):\n",
        "        input[i][l][v][c] = item\n",
        "  output[i][testloader.dataset[i][1]] = 1.0\n",
        "\n",
        "np.save('input', input)\n",
        "np.save('output', output)"
      ],
      "execution_count": 3,
      "outputs": []
    }
  ]
}